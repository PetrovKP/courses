{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pandas as pd\n",
    "from scipy.special import xlogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-Propagation\n",
    "\n",
    "$$\n",
    "X \\rightarrow Z=W_1X \\rightarrow U = W_2Z \\rightarrow S=F_{softmax}(U) \\rightarrow L(S, y) = \\log S_y,\n",
    "$$\n",
    "\n",
    "where $S_y=\\frac{\\exp(U_y)}{\\sum_{j=0}^{K-1}\\exp(U_j)}$ is the y-th element of the $S$ and $U_y$ is the y-th element of the $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward-Propagation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial U_t} =  \n",
    "\\begin{cases}\n",
    "S_t(U), & t\\neq y \\\\\n",
    "1- S_t(U). & t = y\n",
    "\\end{cases}\n",
    "\\Longrightarrow\n",
    "\\frac{\\partial L}{\\partial U} = e_y - S(U), \n",
    "$$\n",
    "\n",
    "where $e_y$ is the unit vector, which y-th coordinate equals to 1 and 0 elsewhere. \n",
    "\n",
    "\\begin{align}\n",
    "& \\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial U}\\frac{\\partial U}{\\partial W_1} = (e_y - S(U))Z^T \\\\\n",
    "& \\frac{\\partial L}{\\partial W_1} =  \\big(\\frac{\\partial L}{\\partial W_2} \\cdot Z\\big)X^T\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_layer_sizes=(100), solver='sgd',\n",
    "                 batch_size=1, learning_rate=0.001, momentum=0.9, eps=1e-8,\n",
    "                 max_iter=200, random_state=32, verbose=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __stable_softmax(self, X):\n",
    "        tmp = X - X.max(axis=1)[:, np.newaxis]\n",
    "        np.exp(tmp, out=X)\n",
    "        X /= X.sum(axis=1)[:, np.newaxis]\n",
    "        return X\n",
    "\n",
    "    def __crossentropy_loss(self, y_true, y_prob):\n",
    "        mask = y_prob > 0\n",
    "        div = 1.0/float(y_prob.shape[0])\n",
    "        y_true = y_true[mask]\n",
    "        y_prob = y_prob[mask]\n",
    "        return - xlogy(y_true, y_prob).sum() * div\n",
    "\n",
    "    def __forward_layer(self, x, w, activation_function):\n",
    "        out = np.dot(x, w)\n",
    "        if activation_function is not None:\n",
    "            out = activation_function(out)\n",
    "        return out\n",
    "\n",
    "    def __forward_propagate(self, x):\n",
    "        network = self.weights\n",
    "        activations = [x]\n",
    "        for in_layer, activataion in zip(network, self.functions):\n",
    "            out = self.__forward_layer(activations[-1], in_layer, activataion)\n",
    "            activations.append(out)\n",
    "        return activations\n",
    "\n",
    "    def __back_propagation(self, activations, y):\n",
    "        network = self.weights\n",
    "        coef_grads = [np.empty_like(a_layer) for a_layer in network]\n",
    "\n",
    "        for i in range(len(network)-1, -1, -1):\n",
    "            deltas = activations[-1] - y if i == len(network)-1 else np.dot(deltas, network[i + 1].T)\n",
    "            coef_grads[i] = np.dot(activations[i].T, deltas)\n",
    "\n",
    "        return coef_grads\n",
    "\n",
    "    def _init_layer(self, input_size, output_size):\n",
    "        w = np.random.randn(input_size, output_size)\n",
    "        return w\n",
    "\n",
    "    def fit(self, X, y, shuffle=False):\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        self._label_binarizer = LabelBinarizer()\n",
    "        y_train = y\n",
    "        X_train = X\n",
    "        y = self._label_binarizer.fit_transform(y)\n",
    "        self._num_classes = len(self._label_binarizer.classes_)\n",
    "\n",
    "        n, p = X.shape\n",
    "        s = self.hidden_layer_sizes[0]\n",
    "\n",
    "        weights = [\n",
    "            self._init_layer(p, s),\n",
    "            self._init_layer(s, self._num_classes)\n",
    "        ]\n",
    "\n",
    "        self.functions = [\n",
    "            None,\n",
    "            self.__stable_softmax,\n",
    "        ]\n",
    "        self.weights = weights\n",
    "        accum_grad = [np.zeros_like(param) for param in weights]\n",
    "\n",
    "        for j in range(self.max_iter):\n",
    "            accumulated_loss = 0.0\n",
    "\n",
    "            if shuffle:\n",
    "                indices = np.arange(n)\n",
    "                np.random.shuffle(indices)\n",
    "                X = X.take(indices, axis=0)\n",
    "                y = y.take(indices, axis=0)\n",
    "            \n",
    "            for i in range(0, n, self.batch_size):\n",
    "                X_batch = X[i : i + self.batch_size]\n",
    "                y_batch = y[i : i + self.batch_size]\n",
    "\n",
    "                activations = self.__forward_propagate(X_batch)\n",
    "\n",
    "                y_prob = activations[-1]\n",
    "\n",
    "                accumulated_loss += self.__crossentropy_loss(y_batch, y_prob)\n",
    "                coef_grads = self.__back_propagation(activations, y_batch)\n",
    "\n",
    "                # update weights (adagrad method)\n",
    "                coef_grads = [grad / self.batch_size for grad in coef_grads]\n",
    "                accum_grad = [accum + grad**2 for accum, grad in zip(accum_grad, coef_grads)]\n",
    "                inv_accum_grad = [self.learning_rate / np.sqrt(self.eps + accum) for accum in accum_grad]\n",
    "                self.weights = [weight - inv_accum * grad for weight, inv_accum, grad in zip(self.weights, inv_accum_grad, coef_grads)]\n",
    "\n",
    "            if self.verbose:\n",
    "                loss = accumulated_loss / X.shape[0]\n",
    "                y_pred = self.predict(X_train)\n",
    "                accuracy = (y_pred == y_train).mean()\n",
    "                print(\"Epoch {}/{};\\t Train accuracy: {:.3f} \\t Loss : {:.3f}\".format(j + 1, self.max_iter, accuracy, loss))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations = self.__forward_propagate(X)\n",
    "        y_pred = activations[-1]\n",
    "        return self._label_binarizer.inverse_transform(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  (60000, 784) (60000,)\n",
      "test size:  (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "data_train = pd.read_csv(\"../dataset/mldata/mnist_train.csv\", header=None)\n",
    "data_test = pd.read_csv(\"../dataset/mldata/mnist_test.csv\", header=None)\n",
    "\n",
    "x_train = np.ascontiguousarray(data_train[data_train.columns[:-1]].values, dtype=np.float32)\n",
    "y_train = np.ascontiguousarray(data_train[data_train.columns[-1]].values, dtype=np.float32)\n",
    "x_test = np.ascontiguousarray(data_test[data_test.columns[:-1]].values, dtype=np.float32)\n",
    "y_test = np.ascontiguousarray(data_test[data_test.columns[-1]].values, dtype=np.float32)\n",
    "\n",
    "print('train size: ', x_train.shape, y_train.shape)\n",
    "print('test size: ', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search parameters for Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-bbbde7eeed24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Лучший подбор параметра для DNNClassifier: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Лучший scope для DNNClassifier: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/inn/proj/numerics1/Users/kpetrov/python/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/inn/proj/numerics1/Users/kpetrov/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/inn/proj/numerics1/Users/kpetrov/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/inn/proj/numerics1/Users/kpetrov/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/inn/proj/numerics1/Users/kpetrov/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/inn/proj/numerics1/Users/kpetrov/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/inn/proj/numerics1/Users/kpetrov/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/inn/proj/numerics1/Users/kpetrov/python/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/inn/proj/numerics1/Users/kpetrov/python/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-847c1ff601bd>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, shuffle)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__forward_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-847c1ff601bd>\u001b[0m in \u001b[0;36m__forward_propagate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0min_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivataion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__forward_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivataion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-847c1ff601bd>\u001b[0m in \u001b[0;36m__forward_layer\u001b[0;34m(self, x, w, activation_function)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__forward_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactivation_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'hidden_layer_sizes': [(64,), (128,), (256,), (512,), (1024,), (2048,)],\n",
    "    'learning_rate': [0.5, 0.1, 0.05],\n",
    "    'batch_size': [256, 512, 1024]\n",
    "}\n",
    "estimator = DNNClassifier(hidden_layer_sizes=(32, ), solver='adagrad',\n",
    "     batch_size=256, learning_rate=0.1, max_iter=20,\n",
    "     random_state=777, verbose=False)\n",
    "\n",
    "clf = GridSearchCV(estimator, parameters, cv=5, scoring='accuracy')\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"Лучший подбор параметра для DNNClassifier: {}\".format(clf.best_params_))\n",
    "print(\"Лучший scope для DNNClassifier: {}\".format(clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNNClassifier(batch_size=256, eps=1e-08, hidden_layer_sizes=(64,),\n",
       "       learning_rate=0.5, max_iter=50, momentum=0.9, random_state=777,\n",
       "       solver=None, verbose=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = clf.best_estimator_\n",
    "up_params = {'verbose': True, 'max_iter': 30}\n",
    "best_estimator.set_params(**up_params)\n",
    "best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50;\t Train accuracy: 0.903 \t Loss : 0.008\n",
      "Epoch 2/50;\t Train accuracy: 0.908 \t Loss : 0.014\n",
      "Epoch 3/50;\t Train accuracy: 0.896 \t Loss : 0.022\n",
      "Epoch 4/50;\t Train accuracy: 0.908 \t Loss : 0.030\n",
      "Epoch 5/50;\t Train accuracy: 0.907 \t Loss : 0.034\n",
      "Epoch 6/50;\t Train accuracy: 0.908 \t Loss : 0.039\n",
      "Epoch 7/50;\t Train accuracy: 0.908 \t Loss : 0.042\n",
      "Epoch 8/50;\t Train accuracy: 0.907 \t Loss : 0.049\n",
      "Epoch 9/50;\t Train accuracy: 0.904 \t Loss : 0.051\n",
      "Epoch 10/50;\t Train accuracy: 0.910 \t Loss : 0.056\n",
      "Epoch 11/50;\t Train accuracy: 0.909 \t Loss : 0.059\n",
      "Epoch 12/50;\t Train accuracy: 0.908 \t Loss : 0.062\n",
      "Epoch 13/50;\t Train accuracy: 0.907 \t Loss : 0.067\n",
      "Epoch 14/50;\t Train accuracy: 0.908 \t Loss : 0.068\n",
      "Epoch 15/50;\t Train accuracy: 0.905 \t Loss : 0.071\n",
      "Epoch 16/50;\t Train accuracy: 0.905 \t Loss : 0.073\n",
      "Epoch 17/50;\t Train accuracy: 0.904 \t Loss : 0.075\n",
      "Epoch 18/50;\t Train accuracy: 0.901 \t Loss : 0.076\n",
      "Epoch 19/50;\t Train accuracy: 0.905 \t Loss : 0.078\n",
      "Epoch 20/50;\t Train accuracy: 0.902 \t Loss : 0.078\n",
      "Epoch 21/50;\t Train accuracy: 0.906 \t Loss : 0.081\n",
      "Epoch 22/50;\t Train accuracy: 0.901 \t Loss : 0.080\n",
      "Epoch 23/50;\t Train accuracy: 0.909 \t Loss : 0.082\n",
      "Epoch 24/50;\t Train accuracy: 0.893 \t Loss : 0.082\n",
      "Epoch 25/50;\t Train accuracy: 0.892 \t Loss : 0.081\n",
      "Epoch 26/50;\t Train accuracy: 0.901 \t Loss : 0.082\n",
      "Epoch 27/50;\t Train accuracy: 0.901 \t Loss : 0.082\n",
      "Epoch 28/50;\t Train accuracy: 0.903 \t Loss : 0.082\n",
      "Epoch 29/50;\t Train accuracy: 0.897 \t Loss : 0.079\n",
      "Epoch 30/50;\t Train accuracy: 0.899 \t Loss : 0.081\n",
      "Epoch 31/50;\t Train accuracy: 0.895 \t Loss : 0.079\n",
      "Epoch 32/50;\t Train accuracy: 0.902 \t Loss : 0.082\n",
      "Epoch 33/50;\t Train accuracy: 0.891 \t Loss : 0.081\n",
      "Epoch 34/50;\t Train accuracy: 0.893 \t Loss : 0.078\n",
      "Epoch 35/50;\t Train accuracy: 0.895 \t Loss : 0.078\n",
      "Epoch 36/50;\t Train accuracy: 0.898 \t Loss : 0.080\n",
      "Epoch 37/50;\t Train accuracy: 0.894 \t Loss : 0.078\n",
      "Epoch 38/50;\t Train accuracy: 0.888 \t Loss : 0.078\n",
      "Epoch 39/50;\t Train accuracy: 0.892 \t Loss : 0.075\n",
      "Epoch 40/50;\t Train accuracy: 0.884 \t Loss : 0.074\n",
      "Epoch 41/50;\t Train accuracy: 0.889 \t Loss : 0.075\n",
      "Epoch 42/50;\t Train accuracy: 0.895 \t Loss : 0.075\n",
      "Epoch 43/50;\t Train accuracy: 0.888 \t Loss : 0.076\n",
      "Epoch 44/50;\t Train accuracy: 0.890 \t Loss : 0.071\n",
      "Epoch 45/50;\t Train accuracy: 0.890 \t Loss : 0.072\n",
      "Epoch 46/50;\t Train accuracy: 0.892 \t Loss : 0.072\n",
      "Epoch 47/50;\t Train accuracy: 0.891 \t Loss : 0.072\n",
      "Epoch 48/50;\t Train accuracy: 0.896 \t Loss : 0.069\n",
      "Epoch 49/50;\t Train accuracy: 0.891 \t Loss : 0.069\n",
      "Epoch 50/50;\t Train accuracy: 0.892 \t Loss : 0.068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(batch_size=256, eps=1e-08, hidden_layer_sizes=(64,),\n",
       "       learning_rate=0.5, max_iter=50, momentum=0.9, random_state=777,\n",
       "       solver=None, verbose=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy for testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8812\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_estimator.predict(x_test)\n",
    "print((y_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
